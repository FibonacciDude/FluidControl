learn
    spinningup - when doing baseline and ablative analysis

baselines
    comparison with other rl methods
    different seeds to see brittleness
    comparison of functionally interesting behavior prediction with rand vs. policy
    compare generalization capabilities - to tackle: does this induce negative bias to
    learning?
    model based active exploration
    random distillation networks
    self-supervised intrinsic exploration

    reproducibility - https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf

speed
    .to is bad, device=device is super good
    os.environ["OMP_NUM_THREADS"] = "1"
    High-dimensional state space, random feature space prediction
    How slow and unparallelizable is the entire stack - gpu, cpu, wall, environment's "T"

practical considerations
    big models - tradeoff between more expressive/faster learning, overfitting, more time
    to predict, less data (wall clock wise)
    batch size and learning rate - https://arxiv.org/pdf/1812.06162.pdf
    hyperparams: https://deepmind.com/blog/article/population-based-training-neural-networks
    Parallelization of transformer in this case
    Multiple seeds, randomness
    parallel environments running
    
    tricks? - reset the network if it hasn't improved?
        maybe not because prediction of random is chaotic but not stochastic

    exploration vs exploitation - add randomness to actions (learned mu and fixed std), no randomness, policy stochasticity (learned mu and std)
        I do want it to exploit though, as whatever context + state -> action leads to
        highest reward is the best learning. Assuming that the context + state analysis is
        optimal

        add high T noise parameter and anneal to train - sketchrnn
        Furthermore, the estimate will have less variance - faster training

    sticky actions? - propensity towards null actions
    tensorboard - visualization
    reward normalization - mean and standard deviation, have negative reward for bad
    actions instead of zero (less likely to do them, less saturation in gradient steps for
            learning)
        has the same effect as using the advantage function
    see if the reward varies over orders of magnitude - to take log
    #reward scaling - more stable, may want perfect behavior instead of interesting
    (depending on how well it does with spikes of reward)
    loss function scaling
    gpu spot instances - multi-gpu parallelization
    masks

experimenting
    See capabilities of predictor by how fast it learns on a_t = null.

    What should be the scale of sim for accuracy and low-dimensionality?
        -High sim, low output?

    use "sacred" for experiments
    see which hyperparameters are important
    add noisy tv for certain actions - or is it trivial?


understand
    What kind of compression are language models/transformers doing and how can it be
    measured?
    Test for partial observability training (POMDP formalism)

challenges
    Reduce hyperparameters (like value function approx.) - better rl algo (iterated sup?)
    Do longer running algs run into O(n^2) computational constraint for learning on entire dataset/buffer?
    active learning
        We need active learning from beginning as some dynamics are just fully chaotic
        Especially if it's hierarchical, error dynamics won't be periodic for a fixed
        dynamics system

        time-varying policies - depends on knowledge
        
        understand knowledge
            latent space compression of previous and latent space delta change

architecture
    conv nets for next state prediction - delta x that is localized (resnet?)
        yes, there is an implicit bias about the dynamics with the structure
    What makes GPT so good, and transformers? (anything in gpt not in transf. that makes it suited for text)
    are transformers good for anything else than language?
    visual + transformers - latent space (why no latent?)


